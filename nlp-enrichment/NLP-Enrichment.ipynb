{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aaf3791",
   "metadata": {},
   "source": [
    "# Corpus Processing â€“ Annotation with spaCy\n",
    "1. Why spaCy?\n",
    "2. Which methods do we use?\n",
    "   * Tokenisation\n",
    "   * Lemmatisation\n",
    "   * PoS-Tagging\n",
    "3. How well do they work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa5486-4072-4056-8ad7-811cef4dbeba",
   "metadata": {},
   "source": [
    "## 0. Load libraries for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49be10-5cf3-41bc-83de-0e96b5babcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81998d",
   "metadata": {},
   "source": [
    "## 1. Read in the txt data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf822f",
   "metadata": {},
   "source": [
    "#### 1.1 Set path to corpus directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = Path(r\"../data/txt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd98045",
   "metadata": {},
   "source": [
    "#### 1.2 Read in the files from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3572c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_linewise(corpus_dir: Path) -> OrderedDict[str,str]:\n",
    "    \"\"\"\n",
    "    Reads txt files from a given directory. Returns a dictionary with the filename\n",
    "    as key and the txt file content as value.\n",
    "    :param Path corpus_dir: The directory in which the txt files are saved\n",
    "    :return OrderedDict[str, str]: The file names as keys, the file content as value\n",
    "    \"\"\"\n",
    "    corpus = OrderedDict()\n",
    "    for filepath in corpus_dir.iterdir():\n",
    "        if filepath.suffix == \".txt\":\n",
    "            text = filepath.read_text()\n",
    "            corpus[filepath.name] = text\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus_linewise(corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e804de",
   "metadata": {},
   "source": [
    "**Check**: How many files does the corpus include?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac06177",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d22bb",
   "metadata": {},
   "source": [
    "## 2. Word frequencies with lazy tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07258cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = \" \".join(corpus.values())\n",
    "words = all_texts.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694298bd-8e8f-4570-b053-f3e7b42f77d8",
   "metadata": {},
   "source": [
    "**Check**: What do the word lists look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37fa42-ae7a-4bca-8b01-36cd1444e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d542d5",
   "metadata": {},
   "source": [
    "How big is the corpus (number of words?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b971409",
   "metadata": {},
   "source": [
    "What words occur how often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17108d58-d25d-4c06-952d-9642e982452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_word = input(\"Input a word for which the frequency will be shown: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a590df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies[chosen_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b158ecb",
   "metadata": {},
   "source": [
    "## 3. Annotation with spaCy \n",
    "Overview of spacy model available [here](https://spacy.io/models) \\\n",
    "Load language specific model (selection):\n",
    "* German: 'de_core_news_sm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69caa926-5e4f-4a7f-bc27-3b4967c4b3e6",
   "metadata": {},
   "source": [
    "### 3.1 Setting up the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be051d6-2ded-4762-81eb-4d9f8d70ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52bea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load language specific model\n",
    "nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb49918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude analysis components to improve the processing speed\n",
    "disable_components = ['ner', 'morphologizer', 'attribute_ruler', 'sentencizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bec70c",
   "metadata": {},
   "source": [
    "### 3.2 Annotation of the Texts: Token, Lemma, PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0ac7d-ec7a-41c9-a91f-d12eabe8b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_corpus(corpus: OrderedDict[str, str], disable_components: list[str]) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Annotate a corpus (filename: text) with spacy. Collect the Token, PoS and Lemma information. \n",
    "    Save the annotation information as a pandas DataFrame. \n",
    "    :param OrderedDict[str, str] corpus: The file names as keys, the file content as value\n",
    "    :param list[str] disable_components: spacy components to be diasbled in the annotation process\n",
    "    :return dict[str, pd.DataFrame]: The file name as keys, the annotated text as value\n",
    "    \"\"\"\n",
    "    # list to collect how long the annotation runs take in seconds\n",
    "    took_per_text = []\n",
    "\n",
    "    # define result dict\n",
    "    corpus_annotated = {}\n",
    "    \n",
    "    filename_list = list(corpus.keys())\n",
    "    current = time()\n",
    "    \n",
    "    # iterate over the corpus values, annotate them with spacy\n",
    "    for i, doc in tqdm(enumerate(nlp.pipe(list(corpus.values())[:1], disable=disable_components))):\n",
    "        before = current\n",
    "        current = time()\n",
    "        took_per_text.append(current - before)\n",
    "\n",
    "        # Save the token, PoS and Lemma information to a dictionary\n",
    "        text_annotated = {}\n",
    "        text_annotated['Token'] = [tok.text for tok in doc]\n",
    "        text_annotated['Lemma'] = [tok.lemma_ for tok in doc]\n",
    "        text_annotated['PoS'] = [tok.tag_ for tok in doc]    \n",
    "\n",
    "        # Save the annotation as pandas DataFrame to the result dict\n",
    "        # Key is the current filename\n",
    "        corpus_annotated[filename_list[i]] = pd.DataFrame(text_annotated)\n",
    "\n",
    "    # print corpus size and performance\n",
    "    print(f\"\"\"Processed {len(corpus_annotated)} texts with spacy.\n",
    "    Took {round(np.mean(took_per_text), 4)} seconds per text on average.\n",
    "    Took {round(np.sum(took_per_text) / 60, 4)} minutes in total.\"\"\")\n",
    "\n",
    "    return corpus_annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63b29c-f2c2-41dc-a216-e3f1b46d1a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_annotated = annotate_corpus(corpus, disable_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfd8a4-b8d1-4fd4-ad0f-3dcd881406da",
   "metadata": {},
   "source": [
    "### 3.3 Annotated Text as Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465ba08",
   "metadata": {},
   "source": [
    "**Check**: What do the annotations look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_annotated[list(corpus_annotated.keys())[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8160c5",
   "metadata": {},
   "source": [
    "### 3.4 Word Frequencies with Real Tokenization   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_tokenized = [word for text in corpus_annotated.values() for word in text.Token]\n",
    "len(all_words_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f560071",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokenized_frequencies = Counter(all_words_tokenized)\n",
    "words_tokenized_frequencies[chosen_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402923a",
   "metadata": {},
   "source": [
    "## 4. Save the annotated corpus as conll files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc1fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(r\"../data/conll\")\n",
    "for filepath, text_annotated in corpus_annotated.items():\n",
    "    filepath = Path(filepath)\n",
    "    output_path = output_dir / filepath.with_suffix(\".conll\")\n",
    "    text_annotated.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
