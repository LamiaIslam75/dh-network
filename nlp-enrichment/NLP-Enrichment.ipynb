{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aaf3791",
   "metadata": {},
   "source": [
    "# Korpusverarbeitung – Annotation mit spaCy\n",
    "\n",
    "## Übersicht\n",
    "Im Folgenden wird exemplarisch ein Text (txt-Datei) mit der Bibliothek [spaCy](https://spacy.io) annotiert. Dafür werden folgendene Schritte durchgeführt:\n",
    "1. Einlesen des Texts\n",
    "2. Worthäufigkeiten ohne echte Tokenisierung\n",
    "   * Aufteilen des Texts in Wörter auf Grundlage von Leerzeichen\n",
    "   * Abfrage von Häufigkeiten\n",
    "4. Annotation mit spaCy\n",
    "   * Laden des Sprachmodells\n",
    "   * Analysekomponenten auswählen\n",
    "   * Text annotieren\n",
    "   * Worthäufigkeiten anzeigen\n",
    "5. Annotation speichern\n",
    "6. Prozess für das gesamte Korpus ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d33ac2-97dd-45fd-99b5-dac91fdde243",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Laden von Bibliotheken \n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "import bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81998d",
   "metadata": {},
   "source": [
    "## 1. Einlesen des Texts\n",
    "Um eine Datei mit Python bearbeiten zu können, muss die Datei zuerst ausgewählt werden, d.h der [Pfad](https://en.wikipedia.org/wiki/Path_(computing)) zur Datei wird gesetzt und dann eingelesen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = Path(\"../data/txt/SNP2719372X-19181015-0-0-0-0.txt\")\n",
    "text = text_path.read_text()\n",
    "print(f\"Textauszug:\\n {text[10280:10400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d22bb",
   "metadata": {},
   "source": [
    "## 2. Worthäufigkeiten ohne echte Tokenisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d76a71-2035-4e50-ab55-7604e60deef7",
   "metadata": {},
   "source": [
    "### 2.1 Text in Wörter aufteilen\n",
    "Der einfachste Weg einen Text automatisch in Wörter aufzuteilen, ist anzunehmen, das Wörter durch Leerzeichen getrennt sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07258cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694298bd-8e8f-4570-b053-f3e7b42f77d8",
   "metadata": {},
   "source": [
    "**Prüfen**: Wie sieht die Wortliste aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37fa42-ae7a-4bca-8b01-36cd1444e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d542d5",
   "metadata": {},
   "source": [
    "Wie viele Wörter gibt es insgesamt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4e5d1-4a04-44e9-9acb-d178549ef8a9",
   "metadata": {},
   "source": [
    "Wie zu sehen ist, hat diese Art der \"falschen\" Tokenisierung den Nachteil, dass Satzzeichen nicht von Wörtern abgetrennt werden. \\\n",
    "Die Wortanzahl ist dementsprechend auch nicht akkurat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca89700-b85b-4666-8a2e-0e6fba45d257",
   "metadata": {},
   "source": [
    "### 2.2 Anzeigen von Worthäufigkeiten\n",
    "Auf Grundlage dieser Wortliste kann trotzdem schon eine erste basale Häufigkeitenabfrage erfolgen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51884e11-7320-45f7-af28-949ad23aa6a6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def lookup_word_frequency(word):\n",
    "    return word_frequencies.get(word, 0)\n",
    "\n",
    "word_input = pn.widgets.TextInput(name='Wort', placeholder='Geben Sie ein Wort ein:')\n",
    "\n",
    "@pn.depends(word_input)\n",
    "def update_output(word):\n",
    "    frequency = lookup_word_frequency(word)\n",
    "    return pn.pane.Markdown(f\"#### Die Häufigkeit des Worts **'{word}'** ist: {frequency}\")\n",
    "pn.Column(word_input, update_output).servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b158ecb",
   "metadata": {},
   "source": [
    "## 3. Annotation mit spaCy\n",
    "Um eine präzisere Einteilung in Wörter zu erhalten (Tokenisierung) und um flektierte Wörter aufeinander abbildbar zu machen (Lemmatisierung), wird der Text im folgenden durch die Bibliothek [spaCy](https://spacy.io/) annotiert. Dafür werden folgende Schritte ausgeführt:\n",
    "1. Das sprachspezifische Modell wird geladen. Wir arbeiten mit dem weniger akkuraten aber schnellsten spaCy Modell `de_core_news_sm`. \n",
    "2. Für eine erhöhte Annotationsgeschwindigkeit werden nur bestimmte Analysekomponenten geladen. Dies ist vor allem für größere Textmengen sinnvoll.\n",
    "3. Der Text wird annotiert und die Token sowie die dazugehörigen Lemmata werden extrahiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69caa926-5e4f-4a7f-bc27-3b4967c4b3e6",
   "metadata": {},
   "source": [
    "### 3.1 Sprachmodell laden\n",
    "Das sprachspezifische Modell wird geladen. Es handelt sich dabei um das am wenigsten akkurate aber schnellste Modell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52bea2b",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a88c3-4beb-42ea-9f88-290a92ce7cc0",
   "metadata": {},
   "source": [
    "### 3.2 Analysekomponenten auswählen\n",
    "Es werden einige Analysekomponent wie z. B. das Aufteilen des Texts in Sätze (sentencizer) oder die [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) (ner) ausgeschlossen, da diese für die Tokenisierung und die Lemmatisierung nicht benötigt werden. Der Auschluss der Komponentnen erhöht die Annotationsgeschwindikgeit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb49918",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_components = ['ner', 'morphologizer', 'attribute_ruler', 'sentencizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bec70c",
   "metadata": {},
   "source": [
    "### 3.3 Annotieren der Texte: Token, Lemma\n",
    "Der ausgewählte Text wird mit spaCy annotiert und die Token sowie die dazugehörigen Lemmata werden extrahiert und in einer Tabelle gespeichert. Das Tabellenformat wurde gewählt, da sich darin gut relationale Daten speichern lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800a679-11c9-4aff-a509-ccc63d9fb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = time()\n",
    "doc = nlp(text)\n",
    "text_annotated = {}\n",
    "text_annotated['Token'] = [tok.text for tok in doc]\n",
    "text_annotated['Lemma'] = [tok.lemma_ for tok in doc]\n",
    "text_annotated_df = pd.DataFrame(text_annotated)\n",
    "took = time() - current\n",
    "print(f\"Die Annotation hat {round(took, 2)} Sekunden gedauert.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccb8ff-e04c-4ad1-b00c-5ffd35b9f873",
   "metadata": {},
   "source": [
    "Auszug aus der Tabelle, in der der annotierte Text gespeichert ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a351b8d-a2eb-4f6e-95c8-73b6ba0daed5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "text_annotated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8160c5",
   "metadata": {},
   "source": [
    "### 3.4 Worthäufigkeit mit echter Tokenization   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf9300-6a9b-41f7-8b4b-3fbfb1adea04",
   "metadata": {},
   "source": [
    "Durch die Tokenisierung wurden z. B. Satzzeichen von Wörtern abgetrennt. An der Textlänge lässt sich dies schon erkennen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenized = text_annotated_df.Lemma\n",
    "len(text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ab432-cfce-4599-ac77-35d72176cca4",
   "metadata": {},
   "source": [
    "Auf Grundlage des tokenisierten und lemmatisierten Texts, kann die Häufigkeitenabfrage erneut augeführt werden. Da durch die Lemmatisierung flektierte Wortformen auf die Grundformen zurückgeführt wurden, erwarten wir, dass die Häufigkeit einer Wortgrundform im Gegensatz zur vorherigen Abfrage erhöht ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098dae70-cb7d-46c6-a2f0-58488c28aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_frequencies = Counter(text_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b30568-4722-418f-8318-83517216b10e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def lookup_token_frequency(token):\n",
    "    return token_frequencies.get(token, 0)\n",
    "\n",
    "token_input = pn.widgets.TextInput(name='Token', placeholder='Geben Sie ein Wort ein:')\n",
    "\n",
    "@pn.depends(token_input)\n",
    "def update_output_token(token):\n",
    "    frequency = lookup_token_frequency(token)\n",
    "    return pn.pane.Markdown(f\"#### Die Häufigkeit des Token **'{token}'** ist: {frequency}\")\n",
    "pn.Column(token_input, update_output_token).servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402923a",
   "metadata": {},
   "source": [
    "## 4. Annotation speichern\n",
    "Um den annotierten Text zu speichern, wird zuerst der Dateiname festgelegt. Dafür wird die Dateiendung ersetzt von `.txt` zu `.csv`.\n",
    "\n",
    "[CSV](https://en.wikipedia.org/wiki/Comma-separated_values) (comma-separated value) ist das Standardformat um tabellarische Daten im Klartext zu speichern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc1fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(r\"../data/csv\") / text_path.with_suffix(\".csv\").name\n",
    "text_annotated_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d865508-e103-4639-9d2b-a3d966aa3f67",
   "metadata": {},
   "source": [
    "## 5. Prozess für das gesamte Korpus ausführen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f7a78-d268-4b7f-b4fa-0ee9502094b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_linewise(corpus_dir: Path) -> OrderedDict[str,str]:\n",
    "    \"\"\"\n",
    "    Reads txt files from a given directory. Returns a dictionary with the filename\n",
    "    as key and the txt file content as value.\n",
    "    :param Path corpus_dir: The directory in which the txt files are saved\n",
    "    :return OrderedDict[str, str]: The file names as keys, the file content as value\n",
    "    \"\"\"\n",
    "    corpus = OrderedDict()\n",
    "    for filepath in corpus_dir.iterdir():\n",
    "        if filepath.suffix == \".txt\":\n",
    "            text = filepath.read_text()\n",
    "            corpus[filepath.name] = text\n",
    "    return corpus\n",
    "\n",
    "def annotate_corpus(corpus: OrderedDict[str, str], disable_components: list[str]) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Annotate a corpus (filename: text) with spacy. Collect the Token and Lemma information. \n",
    "    Save the annotation information as a pandas DataFrame. \n",
    "    :param OrderedDict[str, str] corpus: The file names as keys, the file content as value\n",
    "    :param list[str] disable_components: spacy components to be diasbled in the annotation process\n",
    "    :return dict[str, pd.DataFrame]: The file name as keys, the annotated text as value\n",
    "    \"\"\"\n",
    "    # list to collect how long the annotation runs take in seconds\n",
    "    took_per_text = []\n",
    "\n",
    "    # define result dict\n",
    "    corpus_annotated = {}\n",
    "    \n",
    "    filename_list = list(corpus.keys())\n",
    "    current = time()\n",
    "    \n",
    "    # iterate over the corpus values, annotate them with spacy\n",
    "    for i, doc in tqdm(enumerate(nlp.pipe(list(corpus.values()), disable=disable_components))):\n",
    "        before = current\n",
    "        current = time()\n",
    "        took_per_text.append(current - before)\n",
    "\n",
    "        # Save the token and lemma information to a dictionary\n",
    "        text_annotated = {}\n",
    "        text_annotated['Token'] = [tok.text for tok in doc]\n",
    "        text_annotated['Lemma'] = [tok.lemma_ for tok in doc]\n",
    "\n",
    "        # Save the annotation as pandas DataFrame to the result dict\n",
    "        # Key is the current filename\n",
    "        corpus_annotated[filename_list[i]] = pd.DataFrame(text_annotated)\n",
    "\n",
    "    # print corpus size and performance\n",
    "    print(f\"\"\"Processed {len(corpus_annotated)} texts with spacy.\n",
    "    Took {round(np.mean(took_per_text), 4)} seconds per text on average.\n",
    "    Took {round(np.sum(took_per_text) / 60, 4)} minutes in total.\"\"\")\n",
    "\n",
    "    return corpus_annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7cbfa-5285-43c6-a757-6fc8645d1489",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Korpus einlesen\n",
    "corpus_dir = Path(r\"../data/txt/\")\n",
    "corpus = read_corpus_linewise(corpus_dir)\n",
    "print(f\"Es wurden {len(corpus)} Dateien eingelesen.\")\n",
    "\n",
    "# Korpus annotieren\n",
    "corpus_annotated = annotate_corpus(corpus, disable_components)\n",
    "\n",
    "# Annotiertes Korpus speichern\n",
    "output_dir = Path(r\"../data/csv\")\n",
    "for filepath, text_annotated in corpus_annotated.items():\n",
    "    filepath = Path(filepath)\n",
    "    output_path = output_dir / filepath.with_suffix(\".csv\")\n",
    "    text_annotated.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
