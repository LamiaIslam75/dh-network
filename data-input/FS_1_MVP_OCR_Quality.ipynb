{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9efa76",
   "metadata": {},
   "source": [
    "## Evaluate OCR engine quality\n",
    "\n",
    "Before processing all files, we evaluate OCR quality on a sample image part for evaluation(source: [Deutsche Zeitung, Ausgaben am Montag, 23.12.1918](https://zefys.staatsbibliothek-berlin.de/kalender/auswahl/date/1918-12-23/30744015/)):\n",
    "![sample.jpg](sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d817e",
   "metadata": {},
   "source": [
    "Let us OCR it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a636708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_output = pytesseract.image_to_string(Image.open('sample.jpg'), lang='frk')  # using German fraktur OCR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ocr_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328b4ad",
   "metadata": {},
   "source": [
    "#### 2.1.1 Manually create  the 'ground truth' to evaluate against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a68c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = input('Please insert corrected string: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90f22d",
   "metadata": {},
   "source": [
    "#### 2.1.2 Measure OCR precision, recall and F-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c66de",
   "metadata": {},
   "source": [
    "In the context of Optical Character Recognition (OCR), precision, recall, and F-measure are metrics used to evaluate the accuracy and efficiency of OCR systems in converting images of typed, handwritten, or printed text into machine-encoded text. These metrics help to understand how well an OCR system performs, especially in terms of correctly identifying characters, words, or specific information within documents. Here's how these metrics apply to OCR quality evaluation:\n",
    "\n",
    "###### Precision in OCR\n",
    "In OCR, precision measures the accuracy of the recognized text against the actual text in the document images. It calculates the proportion of correctly identified characters or words out of all the characters or words that the OCR system identified. High precision means that most of the text the OCR system identified as present in the document was actually correct, indicating fewer false positives (i.e., incorrectly identified as present).\n",
    "\n",
    "![](precision.png)\n",
    "\n",
    "##### Recall in OCR\n",
    "Recall in the context of OCR measures the OCR system's ability to capture all the relevant characters or words from the document images. It is the ratio of the correctly identified characters or words to all the characters or words that are actually present in the documents. High recall indicates that the OCR system is able to identify most of the actual text present, minimizing false negatives (i.e., failing to recognize text that is there).\n",
    "\n",
    "![](recall.png)\n",
    "\n",
    "##### F-measure (F1 Score) in OCR\n",
    "The F-measure or F1 score in OCR provides a single metric that combines both precision and recall to give a balanced view of the OCR system's overall performance. Since precision and recall have a trade-off (improving one can often lead to a reduction in the other), the F1 score helps to evaluate the OCR system's effectiveness at recognizing text accurately while minimizing both false positives and false negatives.\n",
    "\n",
    "![](fmeasure.png)\n",
    "\n",
    "These metrics are critical for assessing OCR systems, particularly in applications where the accuracy of text recognition directly impacts the outcome, such as document automation, data extraction from scanned documents, and automated processing of handwritten forms. A balance between high precision and high recall is often desired to ensure that the OCR system is both accurate and comprehensive in its text recognition capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux.measure_ocr_quality import measure_ocr_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03132c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f_score = measure_ocr_quality(ocr_output, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46857ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision: {round(precision, 4)}\\nRecall: {round(recall, 4)}\\nF1-score: {round(f_score, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507d82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
